Acá voy a ir escribiendo texto plano.
En parte va a servir para entrenar este modelo de juguete, y en parte también va a servir para que yo pueda practicar mi ortografía.
Hoy es domingo 30 de noviembre de 2025. Eventualmente, todo terminará.
Quizás intente escribir una oración por línea, pero veré.
No está decidido, y no sé si afecta positiva o negativamente en el aprendizaje del modelo.
Ni siquiera tengo un modelo, todavía.

Bueno, ya tengo una especie de modelo.
No quiero borrar ninguna línea una vez cree una, así que tengo que pensar mejor lo que escribo.
Por ejemplo, técnicamente no tengo ninguna "especie de modelo".
Solo empecé el tokenizador.
No sé si debería agregar este texto al .gitignore, es como que descubran cómo tuiteo.

Recién terminé el dataset, está bueno dentro de todo, aunque claramente se puede complejizar un poco más.
Por ejemplo, lo pensé con un context length fijo, cuando quizás podría hacerse con un rango de context length que llegue hasta un máximo.
No tengo idea de si eso tiene sentido; estoy aprendiendo.

Por otra parte, sigo escribiendo esto sin borrar líneas previas.
Además, una vez aprieto punto, aprieto también enter, pero la verdad no tiene mucho sentido. Suelo contradecirme.
Panchos rosas.

Bueno, volví después de unas semanas de parón. Estuve preparando el recuperatorio de PLP, y posteriormente el final.
Además, estuve jugando al Blasphemous. Muy bueno, una lástima que para poder sacar el 100% de las misiones tendría que haber visitado la tumba de Perpetva antes de pelear con Esdras.
Y para colmo, en el combate con Esdras hicimos kill por kill, o sea, nos matamos mutuamente, así que no pude ver nada luego de matarlo. Directamente entré a la Archicatedral.

Sigo escribiendo cosas porque tengo entendido que el modelo necesita muchísima información para su entrenamiento e, independientemente de que seguramente me convenga generar texto con un LLM, quiero ver primero cuánto se puede lograr con un texto pequeño como este.
Además, estaría bueno ver si empieza a generar texto como yo. Sería una gran ofensa contra mi forma de escribir, la verdad.
Más adelante me gustaría ver si se pueden implementar funcionalidades con Prolog. En la facu dijeron que hace muchos años se veía a Prolog como un lenguaje del futuro, con mucho potencial para inteligencia artificial y procesamiento de lenguaje natural.
Veremos, dijo el ciego.

Piripe Pepino anda por el campo. Lo estoy mirando ahora, mientras escribo esto. Cambié de lugar las cosas de la cocina, y ahora tengo mi escritorio (la mesa verde) pegado a la ventana.
Me olvidé de mencionar que hoy es 1 de enero de 2026. Intenté dormir algo, pero vengo teniendo problemas para conciliar el sueño. Me despierto a las 3, y no me duermo hasta las 6.
Hoy fue peor, porque estuve en lo de mi abuela para las 00.00, me acosté tipo 00.40, dormí hasta las 2, y me quedé acostado pero despierto hasta las 06.00 sin poder dormirme.
Entonces decidí que hoy tiene que ser el último día que me pase esto, por lo que me levanté y arranqué el día (y el año), y la idea es no dormir siesta ni nada por el estilo.
Hace mucho que no hago nada como esto, de pasar muchas horas de vigilia con pocas de sueño, pero es lo que hay. Venceremos.

Esto lo estoy escribiendo en NeoVim. Estoy aprendiendo a usar más la terminal, así que me puse a aprender esto.
Además la personalicé un poco, poniéndole el tema Dracula. Está bastante bien la verdad.

Qué onda zapato, acá estoy de nuevo escribiendo en NeoVim, solo que esta vez con LazyVim en lugar de el NeoVim a pelo. Tengo que configurarlo bien, porque no me está tomando bien algunos keybinds en modo normal, y se ven mal algunas palabras (subrayadas como si fuesen links, aunque quizás sean mas bien errores de idioma).

Un poco de brillitos para salir adelante. (Esa era una prueba para copiar y pegar con Ctrl+Shift+v.)

Esto es una prueba para corregir el rendimiento de LazyGit. En el último commit que hice anduvo muy mal, con mucho lag entre letras al escribir el commit.

Bueno, voy a escribir un poco más. Me gustaría no tener que poner mucho texto generado con IA, pero si es necesario lo voy a hacer. De momento, vemos si alcanza con estas boludeces que escribo cada tanto.

Hoy estoy en lo de Inés, como hace una semana aproximadamente. También me estoy acostumbrando a trabajar con NeoVim y escribir más rápido con la técnica de tener los dedos siempre posicionados en la home row.
Sé que ambas cosas tiene una curva de aprendizaje, pero también estoy en vacaciones, así que si no es ahora no es nunca pienso yo.
Cincuenta líneas de texto son pocas, así que tengo que escribir bastante más. ¿Alcanzará con unas... 200?

Bueno, sobre el proyecto y cómo se viene desarrollando:
Al momento de escribir este texto, estoy empezando con la transformación lineal que hay que hacer por capas. Voy a escribir lo que entiendo que sigue por realizarse no solo para tener más texto, sino para ordenar las ideas y leer más adelante a ver qué tan gagá estaba.

Hasta ahora tenemos el siguiente flujo de transformación de datos: de un txt plano pasamos a un conjunto de tokens. Luego, el struct dataset nos sirve para mapear ventanas de IDs (inputs) a su siguiente token (target). Después, sobre embeddings, lo que hacemos es tomar un input (conjunto de IDs) y "comprimirlo" en un context vector que es la suma de los embeddings de los IDs del input. El siguiente paso será chequear qué token es el más probable dado ese context vector mediante chequearlo en una matriz de pesos (W) y sumarle un vector de sesgo (bias). Esto es la transformación lineal logit = W * h + b, siendo h nuestro context vector.

Suficiente cháchara, vamos a codear un poco. Pero antes, voy a practicar un poco la escritura en el teclado.
